**1.LORA的核心思想是低秩分解——启发来源:矩阵的奇异值（特征值）分解**

​     	<img width="387" height="208" alt="image-20250812110639097" src="https://github.com/user-attachments/assets/f4def223-d235-4364-9fdf-03ba8cad9a95" />

（1）思路：    前四列就已经表示了矩阵的信息。**如果只保留最大的几个特征值(低秩近似)，就能用更少的参数近似W。**

（2）方法：

​	S矩阵（形状 d x k）可以写成：S=U × ∑  x  V^T 

​	其中U 是d x d的正交矩阵，∑是d X k的对角矩阵(奇异值),V^T 是k x k的正交矩阵。

 

​	例如，
<img width="1229" height="488" alt="d163e8a8c43392fd6a9b50b248d10a7" src="https://github.com/user-attachments/assets/5af1cc29-869b-4584-986e-4da026bc12ef" />

​      						

​	选取**最大的特征值**（信息量最多）**重构**

<img width="1228" height="559" alt="image-20250812112207144" src="https://github.com/user-attachments/assets/e6bb9d77-d7d3-49be-a2bf-7ef11f14119a" />

<img width="1073" height="213" alt="image-20250812120405760" src="https://github.com/user-attachments/assets/3c19f9b3-e475-4a75-9b1b-ef40ffb17938" />

计算前后的矩阵行列式的值（通过特征值平方相加）
<img width="1242" height="117" alt="image-20250812112622334" src="https://github.com/user-attachments/assets/b93ef151-3294-4728-9a8a-f81d1784ae4f" />


**2.对增量权重△W  /  原始权重W低 秩 分 解**？



​			**LORA**的**核心公式**可以简单写成:		W'=W+△W=W+ A x B  

​												△W=A×B

（1）Q:为什么可以对增量权重△W 低 秩 分 解 ？

​          A:微调后的权重变化△W的奇异值分解，**大部分信息集中在少数几个特征值上。**在 GPT-3 上测试时发现，**△W 的前10-20个特征值就占据了90%以上的信息**。



（2）Q：为什么不对原始权重W分解？

​         A:一般预训练模型的**原始权重W基本接近满秩**，**特征值平滑（数字大小差别不大）**。



示例：

<img width="1187" height="246" alt="image-20250812114958794" src="https://github.com/user-attachments/assets/79584e91-6d04-4fa3-8230-3f48d62454ab" />


对增量权重△W 分 解 ： 						**△W=A×B**

​            	<img width="869" height="240" alt="image-20250812115722909" src="https://github.com/user-attachments/assets/6e752cdf-eaee-426a-812c-27e8a17d2081" />

（1）对比

原来：△W是5×4矩阵 20个参数。

分解后现在：A是5×2矩阵 10 个参数,A是2×4矩阵 8 个参数。

​		

（2）结论

结论:通过LORA 微调，调参对象从W变为A、B，，使得参数量从20个减少为18个，这是简化的例子。在实际案例中，参数量可以减少为0.01~3%左右。





**3.训练参数更新过程**<img width="1116" height="839" alt="image-20250812120917822" src="https://github.com/user-attachments/assets/3402718e-6eab-4086-a7c4-7cd8bca7866b" />
