1.b站学习深度学习的相关视频

（1）了解卷积、池化、搭建神经网络、优化器、反向传播等;

（2）完整的模型训练流程;

（3）怎么利用GPU训练;


2.阅读论文“Attentive Contextual Attention for Cloud Removal”

（1）背景 

	<img width="892" height="416" alt="image" src="https://github.com/user-attachments/assets/0bb71d3c-1b31-4a0e-ba8f-b9e34489c33c" />

•	传统方法: 基于空间、光谱、时间的技术。但这些方法在面对复杂的云层时往往效果不佳，并且需要大量额外的数据或计算资源。

•	本文方法：AC-Attention是基于CNN 特征，后嵌到一个卷积残差网络（DSen2-CR）框架里使用的。



（2）方法

a.传统注意力机制

<img width="984" height="610" alt="234ca72f9f091f354b9a4ce44db4b89" src="https://github.com/user-attachments/assets/a6e35574-ee40-4d05-b1ea-3f1ed20dd1f3" />


b.本文AC注意力机制

<img width="968" height="537" alt="image" src="https://github.com/user-attachments/assets/0d989808-ddb8-4d8b-8e93-df4088cfcccd" />

<img width="648" height="327" alt="c40cd9152a4e4910207ebe974986d6a" src="https://github.com/user-attachments/assets/fad8ce91-368e-41e9-8b31-34d831ae043e" />



原理：在传统注意力机制中添加——权重模块和偏置模块。这些模块分别负责学习权重 W 和偏置 B，并使用它们在一个线性变换函数中调整相似矩阵。



（3）网络框架ACA-CRNet 

    组成：ACA-CRNet 由 18 层组成，包括 2 层卷积层、14 个 RB 模块和 2 个 RACAB 模块。
    
<img width="1129" height="404" alt="image" src="https://github.com/user-attachments/assets/f2d5190d-1651-4e9c-8c1c-ae0cd49b1917" />

  改动：在现有的DSen2-CR 框架中通过替换两个残差模块（RB），引入了新型的残差 AC 注意力模块（RACAB）。
  
  残差块RB：就是两层 3×3 卷积 + ReLU，把主分支输出按比例系数 α（默认 0.1）缩放后与输入 残差相加。
  
 •	作用：缓解深层网络训练中的退化/梯度消失，让网络更稳定、更容易优化；


 （4）实验
 
   <img width="529" height="454" alt="image" src="https://github.com/user-attachments/assets/1fd300bd-c158-4e2e-badb-8123bef712cf" />

   较低的MAE和SAM值表示更好的重建，而较高的PSNR和SSIM值则反映出更优越的重建性能。
   
   平均绝对误差（MAE）、峰值信噪比（PSNR）、结构相似性指数（SSIM）、光谱角映射（SAM）

   <img width="1075" height="578" alt="image" src="https://github.com/user-attachments/assets/00fe722e-578c-4483-a927-cdbd66d0f0fd" />



  （5）结论与未来
  
  结论：AC-Attention 机制通过动态选择相关特征并排除无关特征，有效提升了云去除的性能。
  
  未来工作： 创建更高效的注意力策略，在捕捉全局特征的同时降低计算成本，以应对高分辨率图像和资源受限环境中注意力机制的平方复杂度。
